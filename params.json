{"name":"Practical Machine Learning Project","tagline":"","body":"## Practical Machine Learning Project\r\n## September 2014\r\n\r\nThis writeup is for my project for the Practical Machine Learning course offered by the Johns Hopkins University School of Biostatistics and Coursera.\r\n\r\n### Executive Summary\r\nThe given datasets which came from http://groupware.les.inf.puc-rio.br/har, consist of data from six participants. The participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions labelled as A, B, C, D, E in the variable 'classe'.\r\n\r\nThe data from accelerameters attached to the participants' belts, forearms, arms and dumbells are recorded while they are performing the biceps curls. The goal of this project is to predict the manner in which they did the exercise.\r\n\r\n3 models (Classiication Tree, Linear Discriminant Analysis and Random Forests) with 4-fold cross validation are used on the training data. Random Forests gives the best in sample error at 0.9997, with out of sample accuracy estimated to be 0.978. \r\n\r\n### Getting and Cleaning Data \r\nThe training and test data sets are downloaded from https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv and https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv and saved in the working directory as \"pml-training.csv\" and \"pml-testing.csv\" respetively. \r\n\r\n```{r}\r\ntrain_data <- read.csv(file=\"pml-training.csv\", header=TRUE, na.strings=c(\"NA\",\"\"))\r\ntest_data <- read.csv(file=\"pml-testing.csv\", header=TRUE, na.strings=c(\"NA\",\"\"))\r\n```\r\nThe training data set consists of `r dim(train_data)[1]` records and `r dim(train_data)[2]` variables, while the test data set consists of `r dim(test_data)[1]` records and `r dim(test_data)[2]` variables. This is a strange as the test data set should have 1 variable less than the traiing data set - the variable \"classe\" which is to be predicted.\r\n\r\n```{r}\r\ndiffCol <- which(!names(train_data)==names(test_data))\r\nnames(train_data)[diffCol]\r\nnames(test_data)[diffCol]\r\n```\r\nIt can be seen that the 2 sets have identical variables except for \"classe\" in the training set and \"problem_id\" in the test set. \r\n\r\n```{r, checking variables}\r\nnames(train_data)[1:10]\r\n```\r\nIt is also noticed that the first 7 variables are not measurements from the accelerometers and hence can be discarded.\r\n\r\nA quick inspection of the data using 'summary' suggests that there are many variables which have NA or empty values and perhaps some which are constants (no variabilty). These variables are discarded from the dataframe. \r\n\r\n```{r, trimming variables}\r\n## delete irrelevant variables, ie columns 1 to 7 \r\ntrain_data <- train_data[,-(1:7)]\r\ntest_data <- test_data[,-(1:7)]\r\n\r\n## remove columns in training set with all NA or empty values \r\nNA_col <- apply(train_data,2,function(x){sum(is.na(x))})\r\ntrain_data <- train_data[, which(NA_col == 0)]\r\ntest_data <- test_data[, which(NA_col == 0)]\r\n\r\n## check for columns in training set with no variability and remove them\r\nlibrary(caret)\r\nno_var <- nearZeroVar(train_data, saveMetrics=TRUE)\r\ntrain_data <- train_data[,no_var$nzv==\"FALSE\"] \r\ntest_data <- test_data[,no_var$nzv==\"FALSE\"] \r\n```\r\n`r dim(train_data)[2]` variables (including 'classe') remain after trimming.\r\n\r\n### Building Model\r\nAs the number of predictor variables is rather large, pre-processing using Principal Component Analysis (PCA) was carried out to try to reduce the number of predictors.\r\n```{r, pre-processing}\r\n## Covert integer data to numeric data\r\ntrain_data[, -53] <- sapply(train_data[, -53], as.numeric)\r\ntest_data[, -53] <- sapply(test_data[, -53], as.numeric)\r\n\r\n## Pre-processing: reduce predictor variables using PCA\r\npreObj <-preProcess(train_data[,-53],method='pca')\r\n\r\ntrain_pca <- predict(preObj, train_data[,-53])\r\ntrain_pca$classe <- train_data$classe\r\n\r\ntest_pca <- predict(preObj, test_data[,-53])\r\ntest_pca$problem_id <- test_data$problem_id\r\n```\r\nThe `r dim(train_data)[2] - 1` predictor variables is reduced to `r dim(train_pca)[2]` components.\r\n\r\nSince the data set is large, it is split into 2 sets, consisting of 75% training set and 25% validation set. \r\n```{r, split data}\r\n## partition into training and cross validation sets\r\nset.seed(20140920)\r\ninTrain <- createDataPartition(train_pca$classe, p=0.75, list=FALSE)\r\ntrainDat <- trainLess1[inTrain,]\r\ncrossVal <- trainLess1[-inTrain,]\r\n```\r\n\r\n3 models are built, using 4-fold cross-validation where possible.\r\n\r\n* Model 1 : Classification Tree \r\n```{r, building model 1}\r\nlibrary(tree)\r\nmodel1 <- tree(classe ~., data=trainDat)\r\ntrain_pred1 <- predict(model1, trainDat, type=\"class\")\r\nconfusionMatrix(train_pred1, trainDat$classe)\r\n```\r\nModel 1 takes seconds to run, with in-sample accuracy of 0.4645, which is rather low. \r\n\r\n* Model 2: Linear Discriminant Analysis\r\n```{r, building model 2}\r\nlibrary(MASS)\r\nmodel2 <- train(classe ~., method=\"lda\", data=trainDat, \r\n                trControl=trainControl(method=\"cv\", number=4))\r\ntrain_pred2 <- predict(model2, trainDat)\r\nconfusionMatrix(train_pred2, trainDat$classe)\r\n```\r\nModel 2 takes seconds to run, with an in-sample accuracy of 0.5312 - higher than Model 1, but still rather low.\r\n\r\n* Model 3: Random Forests\r\n```{r, building model 3}\r\nlibrary(randomForest)\r\nmodel3 <- randomForest(classe ~., data=trainDat)\r\ntrain_pred3 <- predict(model3, trainDat)\r\nconfusionMatrix(train_pred3, trainDat$classe)\r\n```\r\nModel 3 takes about 30 seconds to run, with an  in-sample accuracy of 1, which is the highest amongst the 3 models.\r\n\r\nModel 3 is selected as it has the highest accuracy, with a small tradeoff of taking a slightly longer time to run.\r\n\r\n### Out of Sample Error\r\nTo estimate the out of sample error, the selected model is run on the validation set.\r\n```{r out of sample error}\r\ncross_pred3 <- predict(model3, crossVal)\r\nconfusionMatrix(cross_pred3, crossVal$classe)\r\n```\r\nThe out of sample error is estimated to be 0.978, lower than the in-sample accuracy as expected.\r\n\r\n### Predictions on test data\r\n```{r, predictions}\r\ntest_pred <- predict(model3, test_pca)\r\n```\r\nThe predictions for the test data are `r test_pred`.\r\n19 of the 20 predictions are correct when submitted.\r\n\r\n```{r, write answers for submission, echo=FALSE}\r\n## Write predictions into files for submission\r\npml_write_files = function(x){\r\n    n = length(x)\r\n    for(i in 1:n){\r\n        filename = paste0(\"problem_id_\",i,\".txt\")\r\n        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n    }\r\n}\r\n\r\npml_write_files(test_pred)\r\n```\r\n\r\nReference:\r\n\r\nVelloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.\r\n\r\nRead more: http://groupware.les.inf.puc-rio.br/har#ixzz3DqAExe42\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}